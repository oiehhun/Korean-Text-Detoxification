### Classification ëª¨ë¸ í•™ìŠµ(KcELECTRA) ###

# 1. í™˜ê²½ ì„¤ì •
import pandas as pd
import matplotlib.pyplot as plt

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from warnings import filterwarnings
filterwarnings("ignore")

## 1-1. GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("device:", device)


# 2. ë°ì´í„°ì…‹ ì œì‘
## 2-1. ì›ë³¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("./datasets/merge_dataset.csv", sep=",")

## 2-2. ë°ì´í„° ì „ì²˜ë¦¬
null_idx = df[df.lable.isnull()].index
df.loc[null_idx, "content"]

# lable ì€ contentì˜ ê°€ì¥ ë ë¬¸ìì—´ë¡œ ì„¤ì •
df.loc[null_idx, "lable"] = df.loc[null_idx, "content"].apply(lambda x: x[-1])

# contentëŠ” "\t" ì•ë¶€ë¶„ê¹Œì§€ì˜ ë¬¸ìì—´ë¡œ ì„¤ì •
df.loc[null_idx, "content"] = df.loc[null_idx, "content"].apply(lambda x: x[:-2])

df = df.astype({"lable":"int"})
df.info()


## 2-3. Train set / Test setìœ¼ë¡œ ë‚˜ëˆ„ê¸°
train_data = df.sample(frac=0.8, random_state=42)
test_data = df.drop(train_data.index)

# ë°ì´í„°ì…‹ ê°¯ìˆ˜ í™•ì¸
print('ì¤‘ë³µ ì œê±° ì „ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))
print('ì¤‘ë³µ ì œê±° ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))

# ì¤‘ë³µ ë°ì´í„° ì œê±°
train_data.drop_duplicates(subset=["content"], inplace= True)
test_data.drop_duplicates(subset=["content"], inplace= True)

# ë°ì´í„°ì…‹ ê°¯ìˆ˜ í™•ì¸
print('ì¤‘ë³µ ì œê±° í›„ í•™ìŠµ ë°ì´í„°ì…‹ : {}'.format(len(train_data)))
print('ì¤‘ë³µ ì œê±° í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ : {}'.format(len(test_data)))


## 2-4. í† í¬ë‚˜ì´ì§•
MODEL_NAME = "beomi/KcELECTRA-base-v2022"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

tokenized_train_sentences = tokenizer(
    list(train_data["content"]),
    return_tensors="pt",                # pytorchì˜ tensor í˜•íƒœë¡œ return
    max_length=128,                     # ìµœëŒ€ í† í°ê¸¸ì´ ì„¤ì •
    padding=True,                       # ì œë¡œíŒ¨ë”© ì„¤ì •
    truncation=True,                    # max_length ì´ˆê³¼ í† í° truncate
    add_special_tokens=True,            # special token ì¶”ê°€
    )

print(tokenized_train_sentences[0])
print(tokenized_train_sentences[0].tokens)
print(tokenized_train_sentences[0].ids)
print(tokenized_train_sentences[0].attention_mask)

tokenized_test_sentences = tokenizer(
    list(test_data["content"]),
    return_tensors="pt",
    max_length=128,
    padding=True,
    truncation=True,
    add_special_tokens=True,
    )


## 2-5. ë°ì´í„°ì…‹ ìƒì„±
class CurseDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_label = train_data["lable"].values
test_label = test_data["lable"].values

train_dataset = CurseDataset(tokenized_train_sentences, train_label)
test_dataset = CurseDataset(tokenized_test_sentences, test_label)


# 3. ëª¨ë¸ í•™ìŠµ
## 3-1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
model.to(device)


## 3-2. í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
training_args = TrainingArguments(
    output_dir='./',                    # í•™ìŠµê²°ê³¼ ì €ì¥ê²½ë¡œ
    num_train_epochs=10,                # í•™ìŠµ epoch ì„¤ì •
    per_device_train_batch_size=8,      # train batch_size ì„¤ì •
    per_device_eval_batch_size=64,      # test batch_size ì„¤ì •
    logging_dir='./logs',               # í•™ìŠµlog ì €ì¥ê²½ë¡œ
    logging_steps=500,                  # í•™ìŠµlog ê¸°ë¡ ë‹¨ìœ„
    save_total_limit=2,                 # í•™ìŠµê²°ê³¼ ì €ì¥ ìµœëŒ€ê°¯ìˆ˜ 
)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

trainer = Trainer(
    model=model,                         # í•™ìŠµí•˜ê³ ìí•˜ëŠ” ğŸ¤— Transformers model
    args=training_args,                  # ìœ„ì—ì„œ ì •ì˜í•œ Training Arguments
    train_dataset=train_dataset,         # í•™ìŠµ ë°ì´í„°ì…‹
    eval_dataset=test_dataset,           # í‰ê°€ ë°ì´í„°ì…‹
    compute_metrics=compute_metrics,     # í‰ê°€ì§€í‘œ
)

## 3-3. í•™ìŠµ
trainer.train()


# 4. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
print(f'ì„±ëŠ¥ í‰ê°€ : {trainer.evaluate(eval_dataset=test_dataset)}')