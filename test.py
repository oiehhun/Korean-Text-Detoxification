### End-to-End Test ###

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'NanumGothic'
import warnings
warnings.filterwarnings('ignore')

import shap
import scipy as sp

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate, FewShotPromptTemplate
from langchain.llms import OpenAI
from dotenv import load_dotenv
load_dotenv()

llm = OpenAI()
davinci3= OpenAI(
    model_name = 'text-davinci-003',
    max_tokens = 1000,
    temperature = 0
)

# ============================================< Classification >============================================

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# í† í¬ë‚˜ì´ì§•
MODEL_NAME = "beomi/KcELECTRA-base-v2022"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# ëª¨ë¸ ìƒì„±
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
model.to(device)

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model.load_state_dict(torch.load('./KcELECTRA_5_pth/checkpoint-4500/pytorch_model.bin'))

# ëª¨ë¸ ì˜ˆì¸¡
# 0: curse, 1: non_curse
def sentence_predict(sent):
    # í‰ê°€ëª¨ë“œë¡œ ë³€ê²½
    model.eval()

    # ì…ë ¥ëœ ë¬¸ì¥ í† í¬ë‚˜ì´ì§•
    tokenized_sent = tokenizer(
        sent,
        return_tensors="pt",
        truncation=True,
        add_special_tokens=True,
        max_length=128
    )
    
    # ëª¨ë¸ì´ ìœ„ì¹˜í•œ GPUë¡œ ì´ë™ 
    tokenized_sent.to(device)

    # ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(
            input_ids=tokenized_sent["input_ids"],
            attention_mask=tokenized_sent["attention_mask"],
            token_type_ids=tokenized_sent["token_type_ids"]
            )

    # ê²°ê³¼ return
    logits = outputs[0]
    logits = logits.detach().cpu()
    result = logits.argmax(-1)
    if result == 0:
        result = " >> ì•…ì„±ëŒ“ê¸€ ğŸ‘¿"
    elif result == 1:
        result = " >> ì •ìƒëŒ“ê¸€ ğŸ˜€"
    return result

# ëŒ“ê¸€ ì…ë ¥
samples = pd.read_csv('./datasets/fianl_toxic_dataset.csv', sep=',')
samples = [sample.strip() for sample in samples]
samples = pd.DataFrame(samples, columns=['content'])

# ë¶ˆìš©ì–´ ì œê±°
file_path = './datasets/stopwords.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    # íŒŒì¼ ë‚´ìš©ì„ ì¤„ ë‹¨ìœ„ë¡œ ì½ì–´ì™€ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    lines = file.readlines()
stopwords = [line.strip() for line in lines]


# ============================================< XAI >============================================
# SHAP
# define a prediction function
def f(x):
    tv = torch.tensor([tokenizer.encode(v, pad_to_max_length=True, max_length=128, truncation=True) for v in x]).cuda()
    outputs = model(tv)[0].detach().cpu().numpy()
    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T
    val = sp.special.logit(scores[:,1]) # use one vs rest logit units
    return val

# build an explainer using a token masker
explainer = shap.Explainer(f, tokenizer)


# ============================================< Masking >============================================
original = []
generate = []

for sentence in samples['content']:

    ori = sentence # ì›ë³¸ ë¬¸ì¥
    original.append(ori)

    shap_values = explainer([sentence])

    threshold = sum([i for i in shap_values[0].values if i < 0]) / len([i for i in shap_values[0].values if i < 0])
    
    # ë§ˆìŠ¤í‚¹
    mask_list = []
    shap_values_zip = zip(shap_values[0].values, shap_values[0].data)
    for shap_value, feature in shap_values_zip:
        if shap_value < threshold: # threshold
            if feature.strip() not in stopwords: # stopwords ì œê±°
                if feature.strip().isalnum(): # íŠ¹ìˆ˜ë¬¸ì ì œê±°
                    mask_list.append(feature.strip())
    print('-'*100)
    print('masking list:', mask_list)
    print(f"ì›ë³¸ ë¬¸ì¥ : {sentence}")

    for mask in mask_list:
        sentence = sentence.replace(mask, "[mask]")

    print(f"ë§ˆìŠ¤í‚¹ ë¬¸ì¥ : {sentence}")

    ori_mask = sentence # ë§ˆìŠ¤í‚¹ëœ ë¬¸ì¥


    # ============================================ < LLM > ============================================

    examples = [
        {
            "sentence" : "ë…¸ë ¹ì—°ê¸ˆ ë°›ëŠ” í‹€ë”±ë“¤ì´, ì„¸ê¸ˆë‚´ëŠ” ê°œë¼ì§€ë“¤ë³´ë‹¤ ë§ì•„ì„œ ì˜ë¬¸ì´ë‹¤.",
            "masking_sentence" : "ë…¸ë ¹ì—°ê¸ˆ ë°›ëŠ” [mask], ì„¸ê¸ˆë‚´ëŠ” [mask]ë³´ë‹¤ ë§ì•„ì„œ ì˜ë¬¸ì´ë‹¤.",
            "answer":
        '''
        ë…¸ë ¹ì—°ê¸ˆ ë°›ëŠ” ë…¸ì¸ë“¤ì´, ì„¸ê¸ˆë‚´ëŠ” ì‚¬ëŒë“¤ë³´ë‹¤ ë§ì•„ì„œ ì˜ë¬¸ì´ë‹¤.
        '''
        },
        {
            "sentence" : "ì¢Œë¹¨ ì˜í™” ë‚©ì‹œìš” ê°œë¼ì§€ë“¤ ì„ ë™ì‹œí‚¤ê¸° ë”±ì´ìš”",
            "masking_sentence" : "[mask] ì˜í™” ë‚©ì‹œìš” [mask] [mask]ì‹œí‚¤ê¸° ë”±ì´ìš”",
            "answer":
        '''
        ì¢ŒíŒŒ ì˜í™” ë‚©ì‹œìš”. ì‚¬ëŒë“¤ ë¶€ì¶”ê¸°ê¸° ë”±ì´ìš” 
        '''
        },
        {
            "sentence" : "ìŒì£¼ìš´ì „í•˜ëŠ” ìƒˆë¼ë“¤ì€ ì§„ì§œ ëŒ€ê°€ë¦¬ì— ë­ê°€ ë“ ê±´ì§€... ë‹¤ ë¬´ê¸°ì§•ì—­ ì‹œì¼œë¼",
            "masking_sentence" : "ìŒì£¼ìš´ì „í•˜ëŠ” [mask] ì§„ì§œ [mask] ë­ê°€ ë“ ê±´ì§€... ë‹¤ ë¬´ê¸°ì§•ì—­ ì‹œì¼œë¼",
            "answer":
        '''
        ìŒì£¼ìš´ì „í•˜ëŠ” ì‚¬ëŒë“¤ì€ ì§„ì§œ ë¨¸ë¦¬ì— ë­ê°€ ë“ ê±´ì§€... ë‹¤ ë¬´ê¸°ì§•ì—­ ì‹œì¼œë¼
        '''
        },
        {
            "sentence" : "ëŒ€ê¹¨ë¬¸ì´ ë¬¸ì¬ì¸ í˜‘ë°•ë²”ì„ ì‰´ë“œì¹˜ë„¤? ì—­ì‹œ ëŒ€ê°€ë¦¬ê°€ ë¶•ì–´ì¸ë“¯~",
            "masking_sentence" : "[mask] ë¬¸ì¬ì¸ í˜‘ë°•ë²”ì„ ì‰´ë“œì¹˜ë„¤? ì—­ì‹œ [mask] [mask]~",
            "answer":
        '''
        ë¬¸ì¬ì¸ì§€ì§€ìë“¤ì´ ë¬¸ì¬ì¸ í˜‘ë°•ë²”ì„ ì‰´ë“œì¹˜ë„¤? ì—­ì‹œ ë¨¸ë¦¬ê°€ ë‚˜ìœ ë“¯~
        '''
        }
    ]

    template = """ë„ˆëŠ” ë¬¸ì¥ ìŠ¤íƒ€ì¼ ë³€í™˜ì„ í•˜ëŠ” ì—­í• ì„ í• ê±°ì•¼. 
    ì›ë˜ ëŒ“ê¸€ê³¼, í•´ë‹¹ ëŒ“ê¸€ì´ ì•…ì„± ëŒ“ê¸€ë¡œ íŒë‹¨ë˜ëŠ”ë° ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹œ(=feature importanceê°€ ë†’ì€) ë‹¨ì–´ë¥¼ masking ì²˜ë¦¬í•œ ëŒ“ê¸€ì´ ì£¼ì–´ì§€ë©´ ê¸°ì¡´ ëŒ“ê¸€ì˜ contextëŠ” ìœ ì§€í•˜ë©´ì„œ ì£¼ì–´ì§„ ìŠ¤íƒ€ì¼ì— ë§ëŠ” ëŒ“ê¸€ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ë„ˆì˜ ì„ë¬´ì•¼.
    
    sentence : {sentence} 
    masking_sentence : {masking_sentence}
    ìƒì„± ëŒ“ê¸€: {answer}
    """

    example_prompt = PromptTemplate(
        input_variables=["sentence", "masking_sentence", "answer"], 
        template=template)

    prompt = FewShotPromptTemplate(
        examples=examples,
        example_prompt=example_prompt,
        suffix="sentence: {origin}\nmasking_sentence: {masked}",
        input_variables=["origin", "masked"]
    )

    result = davinci3(
        prompt.format(origin=ori, masked =ori_mask)
    )
    print(result)

    gener = result.split('\n')[-1].strip()
    generate.append(gener)

df = pd.DataFrame({'original':original, 'generate':generate})
df.to_csv('./datasets/test_result.csv', index=False)